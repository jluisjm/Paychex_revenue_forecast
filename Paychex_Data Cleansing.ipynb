{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b13ff44",
   "metadata": {},
   "source": [
    "<h1>Time Series Forecasting Methodology in THG</h1>\n",
    "Time Series Forecasting is a key component in the optimization of business processes as this is part of the planning of diverse operations within a company. In The Hackett Group, we have developed a methodology to analyze time series and generate accurate forecasts by using traditional (statistics-based) and state-of-the-art (artificial intelligence-based) techniques. Statistical techniques apply common approaches to perform forecasting with high accuracy whereas the artificial intelligence methods, use machine learning and deep learning algorithms to perform forecasting with mixed results when compared to results obtained by statistical methods.\n",
    "<h3>Data Acquisition:</h3> Given the goals defined previously, every possible data source that may be relevant is identified and then we construct extraction pipelines to obtain the data needed for the project. Such data can be either internal, external or both. Moreover, we are able to work with other data different than time series that can have an influence in the forecasting tool.\n",
    "<BR>Here we present an example of getting time series data from a Blob repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39822d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os\n",
    "import pandas as pd\n",
    "\n",
    "import yaml\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def load_credentials(credential, file = \"./credentials.yml\"):\n",
    "    \"\"\"\n",
    "    This is an optional step, required only if the server or repository needes authenticated access\n",
    "    :param credential:\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file,\"r\") as c:\n",
    "        credentials = yaml.safe_load(c)[credential]\n",
    "\n",
    "    return credentials\n",
    "\n",
    "def get_blob_list(client, container=\"raw-data\"):\n",
    "    \"\"\"\n",
    "    Get blobs (file names) in a container\n",
    "    This approach can be useful in situations where data is contained in a trusted server \n",
    "    \"\"\"\n",
    "    container_client = client.get_container_client(container)\n",
    "    blob_list = []\n",
    "    for blob in container_client.list_blobs():\n",
    "        file_name = blob.name\n",
    "        blob_list.append(file_name)\n",
    "\n",
    "    return blob_list\n",
    "\n",
    "def upload_df_csv(df, name, client, container=\"clean-data\"):\n",
    "    \"\"\"\n",
    "    This step is performed in cases where data must be uploaded to a repository\n",
    "    :param df:\n",
    "    :param client:\n",
    "    :param container:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    container_client = client.get_container_client(container)\n",
    "\n",
    "    table = df.to_csv(index=False)\n",
    "    blob_client = container_client.upload_blob(name=name,\n",
    "                                               data=table,\n",
    "                                               overwrite=True)\n",
    "\n",
    "    print(\"Uploaded {}\".format(name))\n",
    "\n",
    "    return blob_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1cefb",
   "metadata": {},
   "source": [
    "<h3>Data Cleaning</h3>Data can have anomalies, therefore we perform a cleaning process on this data to fit analysis that will be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67bd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mapping(file_mapping):\n",
    "    return pd.read_csv(file_mapping, encoding=\"latin_1\")\n",
    "\n",
    "\n",
    "def create_date(df):\n",
    "    df_date = df.copy()\n",
    "    #Source data can have different delimiters, this step allows to create different columns from one field\n",
    "    df_date[['Scenario', 'Version', 'Fiscal Year', 'Period']] = df_date['variable'].str.split(\"|\", expand=True)\n",
    "\n",
    "    df_date = df_date[df_date['Period'] != 'YearTotal']\n",
    "\n",
    "    #Each time series must identify when the cycle begins, since there are Fiscal and Normal Years\n",
    "    df_date['Period'] = df_date['Period'].replace(\n",
    "        {'\\nJun': '01', '\\nJul': '02', '\\nAug': '03', '\\nSep': '04', '\\nOct': '05', '\\nNov': '06', '\\nDec': '07',\n",
    "         '\\nJan': '08',\n",
    "         '\\nFeb': '09', '\\nMar': '10', '\\nApr': '11', '\\nMay': '12'})\n",
    "\n",
    "    df_date['Calendar Date'] = (pd.to_datetime(df_date['Fiscal Year'].str.slice(2) + df_date['Period'] + '01',\n",
    "                                               format=\"%y%m%d\").dt.to_period('M') - 7) \\\n",
    "        .dt.to_timestamp() \\\n",
    "        .apply(lambda x: x.strftime('%Y%m%d'))\n",
    "\n",
    "    return df_date.drop(columns='variable')\n",
    "\n",
    "\n",
    "def get_df(file, mapping, client=None, container=\"raw-data\"):\n",
    "    \"\"\"\n",
    "    :param client:\n",
    "    :param file:\n",
    "    :param container:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if client is not None:\n",
    "        # Get container client\n",
    "        container_client = client.get_container_client(container)\n",
    "        # Download the file\n",
    "        stream = container_client.download_blob(file, encoding=\"latin-1\").content_as_text(encoding=\"latin-1\")\n",
    "        file = io.StringIO(stream)\n",
    "        print(\"Processing from Azure Storage\")\n",
    "    else:\n",
    "        file = \"./data/raw/\" + file\n",
    "        print(\"Processing {} locally\".format(file))\n",
    "\n",
    "    # Clean the dataframe\n",
    "    df = pd.read_csv(file, sep=\"\\t\", header=[0, 1, 2, 4], encoding=\"latin-1\")\n",
    "    df = df.iloc[:, :186]\n",
    "    column_names = dict(zip(['Unnamed: 0_level_0', 'Unnamed: 1_level_0', 'Unnamed: 2_level_0', 'Unnamed: 3_level_0'],\n",
    "                            ['Level0', 'Product', 'Account', 'Detail']))\n",
    "    df = df.rename(columns=column_names, level=0)\n",
    "    df = df.rename(\n",
    "        columns=dict(zip(['Unnamed: 0_level_1', 'Unnamed: 1_level_1', 'Unnamed: 2_level_1', 'Unnamed: 3_level_1'],\n",
    "                         ['', '', '', ''])),\n",
    "        level=1)\n",
    "    df = df.rename(\n",
    "        columns=dict(zip(['Unnamed: 0_level_2', 'Unnamed: 1_level_2', 'Unnamed: 2_level_2', 'Unnamed: 3_level_2'],\n",
    "                         ['', '', '', ''])),\n",
    "        level=2)\n",
    "    df = df.rename(\n",
    "        columns=dict(zip(['Unnamed: 0_level_3', 'Unnamed: 1_level_3', 'Unnamed: 2_level_3', 'Unnamed: 3_level_3'],\n",
    "                         ['', '', '', ''])),\n",
    "        level=3)\n",
    "    df.columns = df.columns.map('|'.join).str.strip('|')\n",
    "\n",
    "    df = mapping.merge(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def join_all(file_list, file_mapping=\"./data/dictionary/mapping.csv\", blob_service_client=None, container=\"raw-data\"):\n",
    "    #This is an optional step, used to ingest data from the repository (Blob) and merge it into a single local file\n",
    "    mapping = read_mapping(file_mapping)\n",
    "    list_df = []\n",
    "\n",
    "    for name in file_list:\n",
    "        if name in mapping.File.unique():\n",
    "            print(\"Processing: \", name)\n",
    "            df = get_df(name, mapping, client=blob_service_client, container=container)\n",
    "            list_df.append(df)\n",
    "            print(\"{} added from {}\".format(df.shape, name))\n",
    "        else:\n",
    "            print(\"No process for: \", name)\n",
    "\n",
    "    df = pd.concat(list_df) \\\n",
    "        .replace({\",\": \"\", \"%$\": \"\"}, regex=True) \\\n",
    "        .drop(columns='Level0')\n",
    "    print('All files joined')\n",
    "\n",
    "    id_vars = df.columns.values[:5]\n",
    "    value_vars = df.columns.values[5:]\n",
    "    df = pd.melt(df, id_vars=id_vars, value_vars=value_vars, value_name='Value')\n",
    "\n",
    "    df = create_date(df)\n",
    "    df = df[['Calendar Date','Scenario', 'Version', 'Fiscal Year', 'Period', 'File','Product', 'Account', 'Detail','Item','Value']]\n",
    "\n",
    "    df_predictable = df[~df['Item'].str.contains('Drivers')].reset_index(drop=True).fillna(0)\n",
    "    df_drivers = df[df['Item'].str.contains('Drivers')].reset_index(drop=True)\n",
    "\n",
    "    return df_predictable, df_drivers\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # # Load credentials\n",
    "    # credentials = load_credentials(\"blob_storage\")\n",
    "    # # Start client\n",
    "    # blob_service_client = BlobServiceClient.from_connection_string(credentials['conn_string'])\n",
    "\n",
    "    # Get a list of all the files in raw-data\n",
    "    # file_list = get_blob_list(blob_service_client, container=\"raw-data\")\n",
    "    file_list = os.listdir(\"./data/raw\")\n",
    "\n",
    "    # Download and join all data\n",
    "    df_predictable, df_drivers = join_all(file_list)\n",
    "\n",
    "    # Upload to clean data\n",
    "    # upload_df_csv(df_predictable, \"table_predictable.csv\", blob_service_client)\n",
    "    # upload_df_csv(df_drivers, \"table_drivers.csv\", blob_service_client)\n",
    "\n",
    "    # Save clean data in local path\n",
    "    clean_path = \"./data/clean/\"\n",
    "    if not os.path.exists(clean_path):\n",
    "        os.makedirs(clean_path)\n",
    "    print(\"Directory created\")\n",
    "    df_predictable.to_csv(clean_path+\"table_predictable.csv\", index=False)\n",
    "    df_drivers.to_csv(clean_path+\"table_drivers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
