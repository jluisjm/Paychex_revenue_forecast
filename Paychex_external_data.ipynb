{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348a9f51",
   "metadata": {},
   "source": [
    "<h1>Getting data from external Sources</h1>\n",
    "This notebook presents a methodology to retrieve data from external sources and to upload the outcome of the data processing step to a digital repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook, Workbook\n",
    "\n",
    "import yaml\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def load_credentials(credential, file = \"./credentials.yml\"):\n",
    "    \"\"\"\n",
    "    This is an optional step, required only if the server or repository needs authenticated access\n",
    "    :param credential:\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file,\"r\") as c:\n",
    "        credentials = yaml.safe_load(c)[credential]\n",
    "\n",
    "    return credentials\n",
    "\n",
    "def upload_df_parquet(df, name, client, container=\"clean-data\"):\n",
    "    \"\"\"\n",
    "    This step is performed in cases where data from parquet file must be uploaded to a repository\n",
    "    :param df:\n",
    "    :param client:\n",
    "    :param container:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    container_client = client.get_container_client(container)\n",
    "\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    buf = pa.BufferOutputStream()\n",
    "    pq.write_table(table, buf)\n",
    "    blob_client = container_client.upload_blob(name=name,\n",
    "                                               data=buf.getvalue().to_pybytes(),\n",
    "                                               overwrite=True)\n",
    "\n",
    "    print(\"Uploaded {}\".format(name))\n",
    "\n",
    "    return blob_client\n",
    "\n",
    "def upload_df_csv(df, name, client, container=\"clean-data\"):\n",
    "    \"\"\"\n",
    "    This step is performed in cases where data from csv file must be uploaded to a repository\n",
    "    :param df:\n",
    "    :param client:\n",
    "    :param container:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    container_client = client.get_container_client(container)\n",
    "\n",
    "    table = df.to_csv(index=False)\n",
    "    blob_client = container_client.upload_blob(name=name,\n",
    "                                               data=table,\n",
    "                                               overwrite=True)\n",
    "\n",
    "    print(\"Uploaded {}\".format(name))\n",
    "\n",
    "    return blob_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd30e87",
   "metadata": {},
   "source": [
    "<h2>Retrieving data from web-based APIs</h2>\n",
    "It is a common practice to get time series data using APIs that are connected to data sources.\n",
    "<br>Therefore, here we present some methods to connect to three of these sources using their APIs to retrieve their data and use it in a project. The external data comes from:<br>\n",
    "<ul>\n",
    "    <li>U.S. Bureau of Labor Statistics</li><li>U.S. Census Bureau</li><li>Federal Reserve Bank of St. Louis</li>\n",
    "</ul>\n",
    "Commonly, every open data portal provides guidelines for programmers about how to access their data using the APIs, therefore, it is advised to learn how each of them works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d68243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bls_data(seriesid, startyear='2015', endyear='2022'):\n",
    "    \"\"\"\n",
    "    :param seriesid:\n",
    "    :param startyear:\n",
    "    :param endyear:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Names dictionary\n",
    "    series_dic = {\"LNS14000000\": 'UnemploymentRate',\n",
    "                  \"CES0000000001\": \"NFPayrolls_sa\",\n",
    "                  \"CEU0000000001\": \"NFPayrolls_nsa\"}\n",
    "\n",
    "    # Request data\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data = json.dumps({\"seriesid\": seriesid, \"startyear\": startyear, \"endyear\": endyear})\n",
    "    p = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data=data, headers=headers)\n",
    "\n",
    "    # Load data\n",
    "    json_response = json.loads(p.text)\n",
    "    json_data = json_response['Results']['series'][0]['data']\n",
    "\n",
    "    # Data as dataframe\n",
    "    list_df = []\n",
    "    for s in json_response['Results']['series']:\n",
    "\n",
    "        print(\"Processing: \", s['seriesID'])\n",
    "\n",
    "        try:\n",
    "            col_name = series_dic[s['seriesID']]\n",
    "        except:\n",
    "            col_name = s['seriesID']\n",
    "\n",
    "        ser = pd.DataFrame(s['data']) \\\n",
    "            .astype({'value': 'float'}) \\\n",
    "            .drop(columns=['latest', 'footnotes', 'period']) \\\n",
    "            .rename(columns={'value': col_name}) \\\n",
    "            .set_index(['year', 'periodName'])\n",
    "        list_df.append(ser)\n",
    "\n",
    "    df = pd.concat(list_df, axis=1).reset_index()\n",
    "    df['date'] = pd.to_datetime(df['year'] + df['periodName'] + '01', format=\"%Y%B%d\").dt.strftime(\"%Y%m%d\")\n",
    "    df = df.drop(columns=['year', 'periodName'])\n",
    "    #df = df[['date'] + list(filter(lambda x: x != 'date', df.columns.values))]\n",
    "    return df.set_index('date')\n",
    "\n",
    "def get_census_data(startyear='2015'):\n",
    "    '''\n",
    "\n",
    "    :param startyear:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    print(\"Reading census data\")\n",
    "    url = 'http://api.census.gov/data/timeseries/eits/bfs?get=cell_value,time_slot_id&category_code=TOTAL&seasonally_adj&data_type_code=BA_BA&for=US&time=from+{}'\\\n",
    "        .format(startyear)\n",
    "    p = requests.get(url)\n",
    "    data = json.loads(p.text)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
    "    df['date'] = pd.to_datetime(df['time'] + '01', format=\"%Y-%m%d\").dt.strftime(\"%Y%m%d\")\n",
    "    df = df.drop(['time_slot_id','category_code','data_type_code','time','us'], axis=1)\\\n",
    "        .set_index(['date','seasonally_adj'])\\\n",
    "        .unstack(1)['cell_value']\\\n",
    "        .rename(columns={'no': 'BusinessApplications_nsa', 'yes': 'BusinessApplications_sa'})\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_fred_series(id_series):\n",
    "\n",
    "    print(\"Loading series: \", id_series)\n",
    "    url = 'https://api.stlouisfed.org/fred/series/observations?series_id={}&observation_start=2000-01-01&api_key={}&file_type=json' \\\n",
    "        .format(id_series,\"2f07bf8c1db37581bdb4874f8fa68418\")\n",
    "    p = requests.get(url)\n",
    "    json_response = json.loads(p.text)\n",
    "    df = pd.DataFrame(json_response['observations']) \\\n",
    "        .rename(columns={'value': id_series}) \\\n",
    "        .drop(columns=['realtime_start', 'realtime_end'])\n",
    "\n",
    "    df['date'] = pd.to_datetime(df['date'],format=\"%Y-%m-%d\")\n",
    "\n",
    "    df = df.set_index('date') \\\n",
    "        .replace(\".\", np.nan) \\\n",
    "        .astype(float) \\\n",
    "        .resample('MS').mean() \\\n",
    "        .interpolate('time')\n",
    "\n",
    "    df = df[df.index>= '2010-01-01']\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_fred_data(series_dict, write_excel=False, path='external_data.xlsx'):\n",
    "\n",
    "    total_df = []\n",
    "\n",
    "    if write_excel:\n",
    "        book = Workbook()\n",
    "        book.save(path)\n",
    "\n",
    "    for k in series_dict:\n",
    "        series_df = []\n",
    "        print(\"Loading category: \", k)\n",
    "\n",
    "        for s in series_dict[k]:\n",
    "            series_df.append(get_fred_series(s))\n",
    "\n",
    "        df_cat = pd.concat(series_df, axis=1)\n",
    "        df_cat = df_cat.reset_index().rename(columns={'index': 'date'})\n",
    "        df_cat['date'] = df_cat['date'].apply(lambda x: x.strftime('%Y%m%d'))\n",
    "\n",
    "        if write_excel:\n",
    "            book = load_workbook(path)\n",
    "            writer = pd.ExcelWriter(path, engine = 'openpyxl')\n",
    "            writer.book = book\n",
    "            df_cat.to_excel(writer, sheet_name = k, index=False)\n",
    "            book.save(path)\n",
    "            book.close()\n",
    "            print(\"Category {} save in {}.\".format(k, path))\n",
    "\n",
    "        total_df.append(df_cat.set_index('date'))\n",
    "\n",
    "    return pd.concat(total_df, axis=1, keys=series_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1d138",
   "metadata": {},
   "source": [
    "Having established all methods to retrieve data from external sources, a method is created to start the data collection and store it on the local device.\n",
    "<BR>Each of the tags listed here belongs to a specific time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18758aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # # Load credentials\n",
    "    # credentials = load_credentials(\"blob_storage\")\n",
    "    # # Start client\n",
    "    # blob_service_client = BlobServiceClient.from_connection_string(credentials['conn_string'])\n",
    "\n",
    "    # Get unemployment rate\n",
    "    series_dict = {\n",
    "        'Nation Income & Expenditures': ['GDPC1', 'GDPPOT', 'W875RX1', 'PCEC96', 'PSAVERT', 'FYFR', 'FYONET', 'FYFSD',\n",
    "                                         'GFDEBTN'],\n",
    "        'Pop Employment Labor': ['PAYEMS', 'UNRATE', 'ICSA', 'UEMPMEAN', 'JTSJOL', 'AWHMAN', 'AHETPI', 'OPHNFB', 'POP',\n",
    "                                 'CLF16OV', 'CIVPART'],\n",
    "        'Prod & Bus Act': ['INDPRO', 'TCU', 'BUSINV', 'RRSFS', 'ALTSALES', 'DGORDER', 'BUSLOANS', 'TOTALSL', 'CP', 'HOUST',\n",
    "                           'PERMIT','UNDCONTSA'],\n",
    "        'Prices': ['CPIAUCSL', 'PCEPI', 'PCEPILFE', 'GDPDEF', 'PPIFIS', 'WPSFD49207', 'WPSFD4131', 'WPSID62', 'USSTHPI',\n",
    "                   'SPCS20RSA', 'DCOILWTICO', 'GASREGW', 'MHHNGSP'],\n",
    "        'Money Bank Finance': ['BOGMBASE', 'M1SL', 'M2SL', 'SP500', 'DJIA', 'WILL5000IND', 'VIXCLS', 'STLFSI2',\n",
    "                               'BAMLCC0A2AATRIV', 'FF', 'WGS3MO', 'WGS1YR', 'WGS5YR', 'WFII5', 'WGS10YR', 'WFII10', 'WAAA',\n",
    "                               'WBAA', 'MORTGAGE15US', 'MORTGAGE30US', 'DEXUSEU', 'DEXCHUS', 'DEXCAUS']\n",
    "    }\n",
    "    seriesid = [\"LNS14000000\", \"CEU0000000001\", \"CES0000000001\"]\n",
    "    df = get_fred_data(series_dict).droplevel(0, axis=1).reset_index()\n",
    "\n",
    "    # Upload data\n",
    "\n",
    "    #blob_client = upload_df_parquet(df, \"external_data_fred.parquet\", blob_service_client, container='external-data')\n",
    "    # blob_client = upload_df_csv(df,\n",
    "    #                             \"external_data_fred.csv\",\n",
    "    #                             blob_service_client,\n",
    "    #                             container='external-data')\n",
    "    # Save clean data in local path\n",
    "    external_path = \"./data/external/\"\n",
    "    df.to_csv(external_path+\"external_data_fred.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
